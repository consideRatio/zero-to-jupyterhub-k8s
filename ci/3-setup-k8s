#!/bin/bash
# NOTE: The script should be executed as a part of ci/run!

# SCRIPT SUMMARY
# --------------
# 1. Start and await minikube, if required
# 2. Initialize Helm, if required

# FIXME: currently we only check if minikube+helm is ready, not first if
#        minikube is ready and then if also helm is ready.

# Set shell options
# -e : Exit immediately if a command exits with a non-zero status.
# -u : Treat unset variables as an error when substituting.
# -x : Print commands and their arguments as they are executed.
set -eux

# Setup Kubernetes
# ----------------

# Don't use the default profile: this helps to make it clear whats what while
# inspecting virtualbox's VMs.
minikube profile z2jh

if set +u && [[ ! -z "${CICD}" ]] && set -u; then
  # CICD
  echo "starting minikube - CICD mode"
  minikube config set vm-driver none
  minikube config set WantNoneDriverWarning false
  export CHANGE_MINIKUBE_NONE_USER=true

  minikube addons disable dashboard
  SUDO='sudo -E'
else
  # LOCAL
  if (timeout 3 bash -c 'kubectl get pod -n kube-system --selector app=helm,name=tiller | grep 1/1'); then
    echo "minikube cluster was already setup"
    echo "--- you can re-setup the cluster, remove it first with `ci/clean`"
    exit 0
  fi
  echo "starting minikube - LOCAL mode"
  minikube config set vm-driver virtualbox

  SUDO=''
fi;
minikube config set kubernetes-version v${KUBE_VERSION}
minikube config set WantReportErrorPrompt false
minikube config set WantKubectlDownloadMsg false
minikube config set ShowBootstrapperDeprecationNotification false

if [[ ${KUBE_VERSION} == 1.8* ]] || [[ ${KUBE_VERSION} == 1.9* ]] || [[ ${KUBE_VERSION} == 1.10.0* ]]; then
  # WARNING: localkube not supported beyond Kubernetes 1.10.0
    $(echo $SUDO) $(which minikube) start --bootstrapper=localkube --extra-config=apiserver.Authorization.Mode=RBAC
else
  # WARNING: kubeadm requires systemd. systemd isn't available for Ubuntu 14.04
  # (trusty) VMs as used by Travis and CircleCI. It is available on Ubuntu 16.04
  # (xenial) or 18.04 (bionic) VMs though.
    $(echo $SUDO) $(which minikube) start
fi

# IF LOCAL ENVIRONMENT
if set +u && [[ -z "${CICD}" ]] && set -u; then
  echo "update minikube context"
  # minikube update-context

  echo "setup default namespace"
  kubectl config set-context $(kubectl config current-context) --namespace jh
fi;

echo "waiting for kubernetes"
JSONPATH='{range .items[*]}{@.metadata.name}:{range @.status.conditions[*]}{@.type}={@.status};{end}{end}'
until kubectl get nodes -o jsonpath="$JSONPATH" 2>&1 | grep -q "Ready=True"; do
  sleep 1
done
kubectl get nodes

set +e # Allow resources to pre-exist by tolerating errors a while
if set +u && [[ -z "${CICD}" ]] && set -u; then
  # LOCAL
  echo "patch kubernetes dashboard to work"
  echo "--- access it with `minikube dashboard`"
  kubectl create clusterrolebinding add-on-cluster-admin --clusterrole=cluster-admin --serviceaccount=kube-system:default
fi

if ! (kubectl get pod -n kube-system --selector app=helm,name=tiller | grep 1/1); then
  echo "initialize helm"
  kubectl create serviceaccount tiller --namespace kube-system
  kubectl create clusterrolebinding tiller --clusterrole cluster-admin --serviceaccount=kube-system:tiller
  helm init --service-account tiller

  echo "waiting for tiller"
  kubectl --namespace=kube-system rollout status --watch=true deployment/tiller-deploy
  
  # FIXME: It would be better to have the "waiting for tiller" at a later stage
  # where we need to utilize tiller, like in the test that will install the
  # chart. I think we need to wait for till in another way than rollout status,
  # because apparently I still end up tiller being unavailable when I run the
  # helm upgrade command after having waited.
fi
set -e
